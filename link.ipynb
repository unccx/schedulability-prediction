{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/arc/Development/DeepHypergraph/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dhg import Hypergraph\n",
    "from dhg.data import DBLP8k\n",
    "from dhg.models import HGNNPLinkPred\n",
    "from dhg.random import set_seed\n",
    "from dhg.metrics import LinkPredictionEvaluator as Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, hypergraph, negative_hypergraph, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    pos_score = net(X, hypergraph)\n",
    "    neg_score = net(X, negative_hypergraph)\n",
    "\n",
    "    scores = torch.cat([pos_score, neg_score]).squeeze()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    ).to(device)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(net, X, hypergraph, negative_hypergraph, test=False):\n",
    "    net.eval()\n",
    "    pos_score = net(X, hypergraph)\n",
    "    neg_score = net(X, negative_hypergraph)\n",
    "\n",
    "    scores = torch.cat([pos_score, neg_score]).squeeze()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    ).to(device)\n",
    "\n",
    "    if not test:\n",
    "        res = evaluator.validate(labels, scores)\n",
    "    else:\n",
    "        res = evaluator.test(labels, scores)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def load_data(file_path: Path):\n",
    "    hyperedge_list = []\n",
    "    neg_hyperedge_list = []\n",
    "    with open(file_path / \"hyperedges.csv\", \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # 读取每个超边的顶点列表，并将它们添加到 hyperedge_list 中\n",
    "            hyperedge_list.append(row)\n",
    "    \n",
    "    hyperedge_list = [[int(v) for v in edge] for edge in hyperedge_list]\n",
    "\n",
    "    with open(file_path / \"minimal_unschedulable_combinations.csv\", \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            neg_hyperedge_list.append(row) \n",
    "\n",
    "    neg_hyperedge_list = [[int(v) for v in edge] for edge in neg_hyperedge_list]\n",
    "\n",
    "    with open(file_path / \"task_quadruples.csv\", 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        data = [list(map(float, row)) for row in reader]\n",
    "\n",
    "    # 将数据转换为 Tensor\n",
    "    features = torch.tensor(data)\n",
    "\n",
    "    data = {\"hyperedge_list\": hyperedge_list, \"num_edges\" : len(hyperedge_list)}\n",
    "    neg_data = {\"hyperedge_list\": neg_hyperedge_list, \"num_edges\" : len(neg_hyperedge_list)}\n",
    "\n",
    "    return {\"pos\":data, \"neg\": neg_data, \"vertices_feature\" : features, \"num_vertices\" : features.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2021)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"auc\"])\n",
    "data = load_data(Path(\"../EDF/data/data_s2233_p10_t21/\"))\n",
    "\n",
    "X = data[\"vertices_feature\"]\n",
    "HG = Hypergraph(data[\"num_vertices\"], data[\"pos\"][\"hyperedge_list\"])\n",
    "neg_HG = Hypergraph(data[\"num_vertices\"], data[\"neg\"][\"hyperedge_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_sparsity(matrix):\n",
    "    nonzero_elements = np.count_nonzero(matrix)\n",
    "    total_elements = matrix.size\n",
    "\n",
    "    nonzero_ratio = nonzero_elements / total_elements\n",
    "    zero_ratio = 1 - nonzero_ratio\n",
    "\n",
    "    print(f\"非零元素比例：{nonzero_ratio:.2%}\")\n",
    "    print(f\"零元素比例：{zero_ratio:.2%}\")\n",
    "\n",
    "# calculate_sparsity(HG.H.to_dense().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = HGNNPLinkPred(X.shape[1], 64, 32, use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to(device)\n",
    "HG = HG.to(device)\n",
    "neg_HG = neg_HG.to(device)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: cuda:0\n",
      "HG: cuda\n",
      "neg_HG: cuda\n",
      "net: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"X: {X.device}\")\n",
    "print(f\"HG: {HG.device}\")\n",
    "print(f\"neg_HG: {neg_HG.device}\")\n",
    "print(f\"net: {next(net.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 13.94934s, Loss: 0.71034\n",
      "Epoch: 1, Time: 0.14803s, Loss: 0.70291\n",
      "Epoch: 2, Time: 0.18896s, Loss: 0.69711\n",
      "Epoch: 3, Time: 0.08502s, Loss: 0.68992\n",
      "Epoch: 4, Time: 0.02663s, Loss: 0.68374\n",
      "Epoch: 5, Time: 0.02426s, Loss: 0.68464\n",
      "Epoch: 6, Time: 0.02366s, Loss: 0.67570\n",
      "Epoch: 7, Time: 0.02569s, Loss: 0.68786\n",
      "Epoch: 8, Time: 0.02613s, Loss: 0.66394\n",
      "Epoch: 9, Time: 0.01943s, Loss: 0.67791\n",
      "Epoch: 10, Time: 0.02061s, Loss: 0.67099\n",
      "Epoch: 11, Time: 0.02417s, Loss: 0.66806\n",
      "Epoch: 12, Time: 0.02282s, Loss: 0.65294\n",
      "Epoch: 13, Time: 0.02283s, Loss: 0.65798\n",
      "Epoch: 14, Time: 0.02133s, Loss: 0.65419\n",
      "Epoch: 15, Time: 0.02254s, Loss: 0.64639\n",
      "Epoch: 16, Time: 0.01966s, Loss: 0.64365\n",
      "Epoch: 17, Time: 0.02238s, Loss: 0.63812\n",
      "Epoch: 18, Time: 0.02263s, Loss: 0.67245\n",
      "Epoch: 19, Time: 0.02671s, Loss: 0.64139\n",
      "Epoch: 20, Time: 0.02778s, Loss: 0.61383\n",
      "Epoch: 21, Time: 0.02134s, Loss: 0.61055\n",
      "Epoch: 22, Time: 0.02038s, Loss: 0.63428\n",
      "Epoch: 23, Time: 0.02267s, Loss: 0.60059\n",
      "Epoch: 24, Time: 0.01989s, Loss: 0.60779\n",
      "Epoch: 25, Time: 0.01911s, Loss: 0.61067\n",
      "Epoch: 26, Time: 0.02036s, Loss: 0.59970\n",
      "Epoch: 27, Time: 0.02154s, Loss: 0.57435\n",
      "Epoch: 28, Time: 0.02879s, Loss: 0.55047\n",
      "Epoch: 29, Time: 0.02056s, Loss: 0.55934\n",
      "Epoch: 30, Time: 0.02035s, Loss: 0.57479\n",
      "Epoch: 31, Time: 0.02025s, Loss: 0.55815\n",
      "update best: 0.53792\n",
      "Epoch: 32, Time: 0.02759s, Loss: 0.57905\n",
      "update best: 0.53839\n",
      "Epoch: 33, Time: 0.02530s, Loss: 0.54887\n",
      "Epoch: 34, Time: 0.03181s, Loss: 0.56073\n",
      "Epoch: 35, Time: 0.02547s, Loss: 0.54018\n",
      "Epoch: 36, Time: 0.02144s, Loss: 0.53572\n",
      "Epoch: 37, Time: 0.02421s, Loss: 0.52666\n",
      "Epoch: 38, Time: 0.01989s, Loss: 0.54361\n",
      "Epoch: 39, Time: 0.03225s, Loss: 0.54634\n",
      "Epoch: 40, Time: 0.02093s, Loss: 0.52260\n",
      "Epoch: 41, Time: 0.02140s, Loss: 0.51801\n",
      "Epoch: 42, Time: 0.02035s, Loss: 0.51494\n",
      "Epoch: 43, Time: 0.01885s, Loss: 0.51805\n",
      "Epoch: 44, Time: 0.01968s, Loss: 0.51037\n",
      "Epoch: 45, Time: 0.03036s, Loss: 0.51649\n",
      "Epoch: 46, Time: 0.02239s, Loss: 0.50995\n",
      "Epoch: 47, Time: 0.02659s, Loss: 0.52358\n",
      "Epoch: 48, Time: 0.02575s, Loss: 0.50613\n",
      "Epoch: 49, Time: 0.02057s, Loss: 0.51893\n",
      "Epoch: 50, Time: 0.02596s, Loss: 0.52416\n",
      "Epoch: 51, Time: 0.01954s, Loss: 0.50016\n",
      "Epoch: 52, Time: 0.01961s, Loss: 0.49870\n",
      "Epoch: 53, Time: 0.02277s, Loss: 0.49779\n",
      "Epoch: 54, Time: 0.02055s, Loss: 0.51301\n",
      "Epoch: 55, Time: 0.02904s, Loss: 0.49693\n",
      "Epoch: 56, Time: 0.01969s, Loss: 0.49688\n",
      "Epoch: 57, Time: 0.02727s, Loss: 0.49851\n",
      "Epoch: 58, Time: 0.02373s, Loss: 0.51233\n",
      "Epoch: 59, Time: 0.01897s, Loss: 0.50052\n",
      "Epoch: 60, Time: 0.02392s, Loss: 0.49564\n",
      "Epoch: 61, Time: 0.02818s, Loss: 0.49673\n",
      "Epoch: 62, Time: 0.02769s, Loss: 0.49749\n",
      "Epoch: 63, Time: 0.02220s, Loss: 0.50167\n",
      "Epoch: 64, Time: 0.02033s, Loss: 0.49570\n",
      "Epoch: 65, Time: 0.02104s, Loss: 0.49856\n",
      "Epoch: 66, Time: 0.02182s, Loss: 0.49688\n",
      "Epoch: 67, Time: 0.03183s, Loss: 0.49361\n",
      "Epoch: 68, Time: 0.02058s, Loss: 0.49586\n",
      "Epoch: 69, Time: 0.02078s, Loss: 0.49656\n",
      "Epoch: 70, Time: 0.02562s, Loss: 0.49653\n",
      "Epoch: 71, Time: 0.03196s, Loss: 0.49595\n",
      "Epoch: 72, Time: 0.02819s, Loss: 0.49156\n",
      "Epoch: 73, Time: 0.04094s, Loss: 0.49855\n",
      "Epoch: 74, Time: 0.02563s, Loss: 0.49379\n",
      "Epoch: 75, Time: 0.02918s, Loss: 0.49807\n",
      "Epoch: 76, Time: 0.02108s, Loss: 0.49451\n",
      "Epoch: 77, Time: 0.02384s, Loss: 0.50132\n",
      "Epoch: 78, Time: 0.02452s, Loss: 0.49606\n",
      "Epoch: 79, Time: 0.02193s, Loss: 0.49525\n",
      "Epoch: 80, Time: 0.01920s, Loss: 0.49275\n",
      "Epoch: 81, Time: 0.02209s, Loss: 0.49381\n",
      "Epoch: 82, Time: 0.01997s, Loss: 0.49540\n",
      "Epoch: 83, Time: 0.01913s, Loss: 0.49260\n",
      "Epoch: 84, Time: 0.02775s, Loss: 0.49535\n",
      "Epoch: 85, Time: 0.02148s, Loss: 0.49780\n",
      "Epoch: 86, Time: 0.02012s, Loss: 0.49668\n",
      "Epoch: 87, Time: 0.01960s, Loss: 0.50152\n",
      "Epoch: 88, Time: 0.02028s, Loss: 0.49359\n",
      "Epoch: 89, Time: 0.02564s, Loss: 0.49780\n",
      "Epoch: 90, Time: 0.02131s, Loss: 0.50091\n",
      "Epoch: 91, Time: 0.02081s, Loss: 0.49465\n",
      "Epoch: 92, Time: 0.02223s, Loss: 0.49249\n",
      "Epoch: 93, Time: 0.01952s, Loss: 0.49886\n",
      "Epoch: 94, Time: 0.03598s, Loss: 0.49580\n",
      "Epoch: 95, Time: 0.02763s, Loss: 0.49588\n",
      "Epoch: 96, Time: 0.02163s, Loss: 0.49252\n",
      "Epoch: 97, Time: 0.02310s, Loss: 0.49511\n",
      "Epoch: 98, Time: 0.02237s, Loss: 0.49400\n",
      "Epoch: 99, Time: 0.02331s, Loss: 0.49287\n",
      "Epoch: 100, Time: 0.02224s, Loss: 0.49356\n",
      "Epoch: 101, Time: 0.03315s, Loss: 0.49249\n",
      "Epoch: 102, Time: 0.02362s, Loss: 0.49911\n",
      "Epoch: 103, Time: 0.02121s, Loss: 0.49841\n",
      "Epoch: 104, Time: 0.01979s, Loss: 0.49193\n",
      "Epoch: 105, Time: 0.02247s, Loss: 0.49180\n",
      "Epoch: 106, Time: 0.02675s, Loss: 0.49294\n",
      "Epoch: 107, Time: 0.01875s, Loss: 0.49193\n",
      "Epoch: 108, Time: 0.01953s, Loss: 0.49332\n",
      "Epoch: 109, Time: 0.01903s, Loss: 0.49101\n",
      "Epoch: 110, Time: 0.01901s, Loss: 0.49219\n",
      "Epoch: 111, Time: 0.02127s, Loss: 0.49387\n",
      "Epoch: 112, Time: 0.02065s, Loss: 0.49156\n",
      "Epoch: 113, Time: 0.02110s, Loss: 0.49407\n",
      "Epoch: 114, Time: 0.02373s, Loss: 0.50174\n",
      "Epoch: 115, Time: 0.02196s, Loss: 0.50128\n",
      "Epoch: 116, Time: 0.01920s, Loss: 0.49156\n",
      "Epoch: 117, Time: 0.03102s, Loss: 0.49221\n",
      "Epoch: 118, Time: 0.02089s, Loss: 0.49165\n",
      "Epoch: 119, Time: 0.02560s, Loss: 0.49127\n",
      "Epoch: 120, Time: 0.01827s, Loss: 0.49090\n",
      "Epoch: 121, Time: 0.01875s, Loss: 0.49142\n",
      "Epoch: 122, Time: 0.01819s, Loss: 0.49586\n",
      "Epoch: 123, Time: 0.01754s, Loss: 0.49121\n",
      "Epoch: 124, Time: 0.01770s, Loss: 0.49149\n",
      "Epoch: 125, Time: 0.02849s, Loss: 0.49162\n",
      "Epoch: 126, Time: 0.02124s, Loss: 0.49494\n",
      "Epoch: 127, Time: 0.02200s, Loss: 0.50365\n",
      "Epoch: 128, Time: 0.02031s, Loss: 0.49441\n",
      "Epoch: 129, Time: 0.02084s, Loss: 0.49571\n",
      "Epoch: 130, Time: 0.02174s, Loss: 0.49221\n",
      "Epoch: 131, Time: 0.01886s, Loss: 0.49143\n",
      "Epoch: 132, Time: 0.01931s, Loss: 0.49559\n",
      "Epoch: 133, Time: 0.02227s, Loss: 0.49340\n",
      "Epoch: 134, Time: 0.02292s, Loss: 0.49329\n",
      "Epoch: 135, Time: 0.02736s, Loss: 0.49233\n",
      "Epoch: 136, Time: 0.03118s, Loss: 0.49433\n",
      "Epoch: 137, Time: 0.02733s, Loss: 0.49496\n",
      "Epoch: 138, Time: 0.01924s, Loss: 0.50557\n",
      "Epoch: 139, Time: 0.01968s, Loss: 0.49246\n",
      "Epoch: 140, Time: 0.01910s, Loss: 0.49606\n",
      "Epoch: 141, Time: 0.01923s, Loss: 0.49534\n",
      "Epoch: 142, Time: 0.02066s, Loss: 0.49887\n",
      "Epoch: 143, Time: 0.02163s, Loss: 0.49171\n",
      "Epoch: 144, Time: 0.02335s, Loss: 0.49284\n",
      "Epoch: 145, Time: 0.02018s, Loss: 0.49552\n",
      "Epoch: 146, Time: 0.01992s, Loss: 0.49390\n",
      "Epoch: 147, Time: 0.01838s, Loss: 0.50156\n",
      "Epoch: 148, Time: 0.01776s, Loss: 0.49057\n",
      "Epoch: 149, Time: 0.01860s, Loss: 0.54941\n",
      "Epoch: 150, Time: 0.01923s, Loss: 0.50107\n",
      "Epoch: 151, Time: 0.02064s, Loss: 0.50761\n",
      "Epoch: 152, Time: 0.01905s, Loss: 0.49132\n",
      "Epoch: 153, Time: 0.01851s, Loss: 0.49257\n",
      "Epoch: 154, Time: 0.02647s, Loss: 0.49360\n",
      "Epoch: 155, Time: 0.02590s, Loss: 0.49900\n",
      "Epoch: 156, Time: 0.02102s, Loss: 0.49352\n",
      "Epoch: 157, Time: 0.02244s, Loss: 0.49725\n",
      "Epoch: 158, Time: 0.02735s, Loss: 0.50467\n",
      "Epoch: 159, Time: 0.01985s, Loss: 0.49595\n",
      "Epoch: 160, Time: 0.01857s, Loss: 0.49399\n",
      "Epoch: 161, Time: 0.02302s, Loss: 0.49369\n",
      "Epoch: 162, Time: 0.01860s, Loss: 0.49358\n",
      "Epoch: 163, Time: 0.02465s, Loss: 0.50235\n",
      "Epoch: 164, Time: 0.02158s, Loss: 0.49266\n",
      "Epoch: 165, Time: 0.02284s, Loss: 0.49708\n",
      "Epoch: 166, Time: 0.02050s, Loss: 0.49284\n",
      "Epoch: 167, Time: 0.02527s, Loss: 0.49217\n",
      "Epoch: 168, Time: 0.03044s, Loss: 0.49195\n",
      "Epoch: 169, Time: 0.01904s, Loss: 0.49815\n",
      "Epoch: 170, Time: 0.01974s, Loss: 0.49525\n",
      "Epoch: 171, Time: 0.02317s, Loss: 0.49275\n",
      "Epoch: 172, Time: 0.01957s, Loss: 0.49518\n",
      "Epoch: 173, Time: 0.01963s, Loss: 0.49205\n",
      "Epoch: 174, Time: 0.02488s, Loss: 0.49306\n",
      "Epoch: 175, Time: 0.01929s, Loss: 0.50174\n",
      "Epoch: 176, Time: 0.02132s, Loss: 0.49313\n",
      "Epoch: 177, Time: 0.02280s, Loss: 0.49687\n",
      "Epoch: 178, Time: 0.02256s, Loss: 0.50402\n",
      "Epoch: 179, Time: 0.01924s, Loss: 0.49437\n",
      "Epoch: 180, Time: 0.02082s, Loss: 0.49313\n",
      "Epoch: 181, Time: 0.02003s, Loss: 0.49316\n",
      "Epoch: 182, Time: 0.01942s, Loss: 0.49385\n",
      "Epoch: 183, Time: 0.03278s, Loss: 0.49756\n",
      "Epoch: 184, Time: 0.01922s, Loss: 0.49513\n",
      "Epoch: 185, Time: 0.03366s, Loss: 0.52966\n",
      "Epoch: 186, Time: 0.02149s, Loss: 0.49288\n",
      "Epoch: 187, Time: 0.01913s, Loss: 0.49189\n",
      "Epoch: 188, Time: 0.01969s, Loss: 0.49294\n",
      "Epoch: 189, Time: 0.01982s, Loss: 0.51128\n",
      "Epoch: 190, Time: 0.01926s, Loss: 0.49519\n",
      "Epoch: 191, Time: 0.02225s, Loss: 0.50298\n",
      "Epoch: 192, Time: 0.02253s, Loss: 0.49382\n",
      "Epoch: 193, Time: 0.01869s, Loss: 0.49601\n",
      "Epoch: 194, Time: 0.01937s, Loss: 0.50415\n",
      "Epoch: 195, Time: 0.01911s, Loss: 0.50694\n",
      "Epoch: 196, Time: 0.01910s, Loss: 0.49227\n",
      "Epoch: 197, Time: 0.03275s, Loss: 0.50893\n",
      "Epoch: 198, Time: 0.02400s, Loss: 0.49312\n",
      "Epoch: 199, Time: 0.02671s, Loss: 0.49657\n",
      "\n",
      "train finished!\n",
      "best val: 0.53839\n"
     ]
    }
   ],
   "source": [
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, HG, neg_HG, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, HG, neg_HG)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...\n",
      "final result: epoch: 32\n",
      "{'accuracy': 0.5383862257003784, 'auc': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, HG, neg_HG, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DHG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
